from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig, pipeline
from PIL import Image
import pyaudio
import wave
import struct
from pvrecorder import PvRecorder
from pyannote.audio import Pipeline
from pydub import AudioSegment
import os
from transformers import WhisperProcessor, WhisperForConditionalGeneration
import torchaudio
import re
from ollama import chat
from ollama import ChatResponse
import torch
import numpy as np
import cv2
import matplotlib.pyplot as plt


# Function to load the model and processor
def load_model_and_processor(model_name):
    """
    Loads the model and processor based on the provided model name.

    Args:
        model_name: The name of the model to load.

    Returns:
        model: The loaded language model.
        processor: The loaded processor.
    """
    processor = AutoProcessor.from_pretrained(
        model_name,
        trust_remote_code=True,
        torch_dtype='auto',
        device_map='auto'
    )

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        trust_remote_code=True,
        torch_dtype='auto',
        device_map='auto'
    )

    return model, processor

# Function to process input and generate output
def generate_output(model, processor, image_path, input_text):
    """
    Processes the input image and text, and generates output using the model.

    Args:
        model: The loaded language model.
        processor: The loaded processor.
        image_path: Path to the input image.
        input_text: Text input for the model.

    Returns:
        generated_text: The text generated by the model.
    """
    # Open the image
    image = Image.open(image_path).convert("RGB")

    # Process the image and text
    inputs = processor.process(
        images=[image],
        text=input_text
    )

    # Move inputs to the correct device and make a batch of size 1
    inputs = {k: v.to(model.device).unsqueeze(0) for k, v in inputs.items()}

    # Generate output
    output = model.generate_from_batch(
        inputs,
        GenerationConfig(max_new_tokens=600, stop_strings=["<|endoftext|>"]),
        tokenizer=processor.tokenizer
    )

    # Decode the generated tokens to text
    generated_tokens = output[0, inputs['input_ids'].size(1):]
    generated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)


    return generated_text

def get_image_dimensions(image_path):
    # Open the image from the local path
    image = Image.open(image_path)
    # Get dimensions
    width, height = image.size
    return width, height





def split_audio_by_speaker(audio_path, diarization, output_folder="Saved Audio/speakers"):
    """
    Splits an audio file into separate files for each speaker based on diarization results.

    Args:
        audio_path (str): Path to the original audio file.
        diarization: Diarization results from the Pyannote pipeline.
        output_folder (str): Directory to save the split audio files. Default is "Saved Audio/speakers".
    """
    # Load the original audio file
    audio = AudioSegment.from_wav(audio_path)

    # Create directories if they don't exist
    os.makedirs(output_folder, exist_ok=True)

    # Initialize empty audio segments for each speaker
    speaker_segments = {}

    # Process each segment and add to corresponding speaker
    for turn, _, speaker in diarization.itertracks(yield_label=True):
        start_ms = turn.start * 1000  # Convert to milliseconds
        end_ms = turn.end * 1000
        segment = audio[start_ms:end_ms]
        
        if speaker not in speaker_segments:
            speaker_segments[speaker] = AudioSegment.empty()
        
        speaker_segments[speaker] += segment

    # Export individual speaker audio files
    for speaker, audio_segment in speaker_segments.items():
        output_path = os.path.join(output_folder, f"{speaker.lower()}.wav")
        audio_segment.export(output_path, format="wav")
        print(f"Saved {speaker} audio to {output_path}")

def transcribe_audio(audio, model_name="openai/whisper-small"):
    """
    Transcribes an audio file using a Whisper model.

    Args:
        audio_path (str): Path to the audio file to transcribe.
        model_name (str): Name of the pre-trained Whisper model. Default is "openai/whisper-large-v2".

    Returns:
        str: Transcribed text from the audio.
    """
    # Load model and processor
    processor = WhisperProcessor.from_pretrained(model_name)
    model = WhisperForConditionalGeneration.from_pretrained(model_name)
    model.config.forced_decoder_ids = None

    # # Load the audio file
    # waveform, sampling_rate = torchaudio.load(audio_path)

    # # Resample the audio if needed
    # target_sampling_rate = processor.feature_extractor.sampling_rate
    # if sampling_rate != target_sampling_rate:
    #     resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=target_sampling_rate)
    #     waveform = resampler(waveform)

    # Preprocess the audio
    input_features = processor(audio, return_tensors="pt").input_features

    # Generate token IDs
    predicted_ids = model.generate(input_features)

    # Decode token IDs to text
    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)
    return transcription[0]

def transcribe_long_audio(audio_path, model_name="openai/whisper-small", chunk_length_s=30):
    """
    Transcribes long audio files using a Whisper model with chunking.

    Args:
        audio_path (str): Path to the audio file to transcribe.
        model_name (str): Name of the pre-trained Whisper model. Default is "openai/whisper-small".
        chunk_length_s (int): Length of audio chunks in seconds. Default is 30 seconds.

    Returns:
        str: Complete transcribed text from the audio.
    """
    # Determine the device to use
    device = "cuda:0" if torch.cuda.is_available() else "cpu"

    # Load the pipeline for automatic speech recognition
    pipe = pipeline(
        "automatic-speech-recognition",
        model=model_name,
        chunk_length_s=chunk_length_s,
        device=device,
    )

    # Load the audio file
    waveform, sampling_rate = torchaudio.load(audio_path)

    # Resample the audio if needed
    target_sampling_rate = pipe.feature_extractor.sampling_rate
    if sampling_rate != target_sampling_rate:
        resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=target_sampling_rate)
        waveform = resampler(waveform)

    # Convert waveform to a dictionary format required by the pipeline
    audio_data = {
        "array": waveform.squeeze().numpy(),
        "sampling_rate": target_sampling_rate
    }

    # Transcribe the audio
    transcription = pipe(audio_data, batch_size=8)["text"]

    return transcription

def extract_items_after_hyphen(output):
    """
    Extracts and returns text after hyphens from a given multi-line string.

    Args:
        output (str): Multi-line string containing lines with hyphens.

    Returns:
        list: List of strings that appear after hyphens.
    """
    # Split lines and extract text after hyphens
    lines = output.splitlines()
    items_after_hyphen = [line.split('-', 1)[1].strip() for line in lines if '-' in line]
    return items_after_hyphen

def extract_fields_with_ollama(model_name, fields, transcriptions):
    """
    Use Ollama Llama3.2 to extract relevant fields from transcriptions.

    Args:
        model_name (str): The name of the Ollama model to use.
        fields (list): List of fields to fill.
        transcriptions (list): List of transcription strings.

    Returns:
        str: The response from the model containing extracted information.
    """
    # Combine fields and transcriptions into a single prompt
    fields_text = "\n".join(fields)
    transcriptions_text = "\n".join(transcriptions) if isinstance(transcriptions, list) else transcriptions
    prompt = (
        f"Extract the following fields from the provided transcriptions:\n"
        f"Fields:\n{fields_text}\n"
        f"Transcriptions:\n{transcriptions_text}\n"
        f"Output in a dictionary with field names as keys and extracted values as values. Do not add an extra demarkations of punctuations to make it print pretty."
    )
    # print('prompt:',prompt)
    # Send the request to the Ollama model
    response: ChatResponse = chat(model=model_name, messages=[
        {
            'role': 'user',
            'content': prompt,
        },
    ])

    # Return the response content
    # print('response:',response)
    return response['message']['content']

def extract_points(molmo_output, image_w, image_h):
    all_points = []
    for match in re.finditer(r'x\d*="\s*([0-9]+(?:\.[0-9]+)?)"\s+y\d*="\s*([0-9]+(?:\.[0-9]+)?)"', molmo_output):
        try:
            point = [float(match.group(i)) for i in range(1, 3)]
        except ValueError:
            pass
        else:
            point = np.array(point)
            if np.max(point) > 100:
                # Treat as an invalid output
                continue
            point /= 100.0
            point = point * np.array([image_w, image_h])
            all_points.append(point)
    return all_points


def annotate_form_with_values(image_path, extracted_fields, model, processor, output_path="annotated_form.png"):
    """
    Annotates a form image with values at specified points and saves it.

    Args:
        image_path (str): Path to the image of the form.
        extracted_fields (dict): Dictionary containing field names and their values.
        model: The model used for generating output points.
        processor: The processor used for preprocessing the form.
        output_path (str): Path to save the annotated image. Default is "annotated_form.png".

    Returns:
        None
    """
    w, h = get_image_dimensions(image_path)
    field_point_dict = {}
    img = cv2.imread(image_path)

    for key, value in eval(extracted_fields).items():
        input_text = f"point to the space in the form where '{key}' is can be filled in by someone filling the form."
        output_point = generate_output(model, processor, image_path, input_text)
        points = extract_points(output_point, w, h)
        field_point_dict[key] = points

        # Format the value for display
        if isinstance(value, str):
            text_value = value
        elif isinstance(value, (list, set)):
            text_value = "\n".join(map(str, value))
        elif isinstance(value, dict):
            text_value = "\n".join(f"{k}: {v}" for k, v in value.items())
        else:
            text_value = str(value)

        # Annotate the image with the value
        if points:  # Ensure points are not empty
            cv2.putText(img, text_value, (int(points[0][0]), int(points[0][1])), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)

    # Save the annotated image
    cv2.imwrite(output_path, img)
    print(f"Annotated image saved to {output_path}")